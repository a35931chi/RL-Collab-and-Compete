{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration & Competition (Tennis) Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "One difficult but interesting subject in the world of artificial intelligence is to design agents who can achieve their goals in the most efficient manner. In a previous exercise where we trained robotic arms to follow a target, we used Deep Deterministic Policy Gradient (DDPG), which is an actor critic algorithm that used a Q-function and a policy concurrently. Depending on the environment, there may be opportunities for agents to interact with the environment and each other, which provides opportunities to learn off of each other. As we seen in implementation of Alphazero, not only does this method increased the efficiencies of training, but it also elevated the algo's performance to beat masters in the game of Go. \n",
    "\n",
    "In this exercise, I have been provided with a tennis environment, where agents are able to control rackets that can hit balls back and forth. Agents are awarded and penalized to maximize play time per episode. My goal is to work off of the DDPG algo that I developed in the robotic arm exercise, and train agents that can exceed the goal of scoring more than 0.5 on average for more than 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm\n",
    "\n",
    "#### Implement Learning Algorithm\n",
    "\n",
    "Some key factors to consider before algo selection:\n",
    "\n",
    "- Multiple agents — The Tennis environment has 2 different agents.\n",
    "- Continuous action space — The action space is now continuous, which allows each agent to execute more complex and precise movements. Even though each tennis agent can only move forward, backward, or jump, the range of possible action values that control these movements are endless.\n",
    "\n",
    "We need an algorithm that allows our agents to utilize its full range and power of movement. Policy-based methods seem to be most suitable.\n",
    "\n",
    " \n",
    "To fully immerse in the Multi-Agent Deep Deterministic Policy Gradient (MADDPG), we can talk about the following:\n",
    "\n",
    "1. Actor-Critic Method\n",
    "\n",
    "Actor-critic methods leverage the strengths of both policy-based and value-based methods.\n",
    "\n",
    "With a policy-based approach, the agent (actor) learns how to act by directly estimating the optimal policy and maximizing reward through gradient ascent. The agent (critic) also learns how to estimate the value of different state-action tuples. Actor-critic methods combine these two approaches to accelerate the learning process. Actor-critic agents are more stable than value-based agents, while requiring fewer training samples than policy-based agents.\n",
    "\n",
    "What makes this implementation unique is that the actors leverages a centralized critic approach. Whereas the traditional actor-critic methods have a critic for each agent, this approach utilizes a single critic that receives observations from all agents. This extra information makes training easier and allows for \"server\" training with \"client\" execution. Each agent still takes actions based on its own unique observations of the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exploration vs Exploitation\n",
    "\n",
    "The idea of Exploration is to encourage the agent to take actions that are unknown. With a continuous space, I have chosen the Ornstein-Uhlenbeck noise to work with. Since OU noise is correlated to the previous oise, it tends to stay in the same direction for longer durations without canceling itself out. This property allows the agent to maintain velocity and explore the action space with more continuity. The OU process adds a certain amount of noise to the action values at each timestep. \n",
    "\n",
    "The OU process has several hyperparameters that determine the noise characteristics:\n",
    "\n",
    "- mu: the long-running mean\n",
    "- theta: the speed of mean reversion\n",
    "- sigma: the volatility parameter\n",
    "- eps_start: initial value for epsilon in noise decay process in Agent.act()\n",
    "- eps_ep_end: episode to end the noise decay process\n",
    "- eps_final: final value for epsilon after decay\n",
    "\n",
    "Notice also there's an epsilon parameter used to decay the noise level over time. This decay mechanism ensures that more noise is introduced earlier in the training process and the noise decreases over time as the agent gains more experience. \n",
    "\n",
    "Note: By boosting the noise output from the OU process early on, the algo encouraged aggressive exploration of the action space and improved the chances that some signals would be detected. This extra signal seemed to improve learning later in training once the noise decayed to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Learning Interval\n",
    "\n",
    "Performing multiple learning passes per episode yield faster convergence and higher scores. For example: at each learning step, the algorithm samples experiences from the buffer and runs the Agent.learn() method several times. \n",
    "\n",
    "- learn_every: we perform learning for every learn_every episodes. 1 means learn every episodes\n",
    "- learn_num: number of NN passes per learning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Gradient Clipping\n",
    "\n",
    "Seems that my agent suffered from \"catestropic chronic amnesia\". The algo would pick up some learning, and just went it seems the agent is on the right track, it completely fail to score. As a result, the score moving average takes a nose dive and never recovers.\n",
    "\n",
    "I suspect that one of the causes was outsized gradients. I implemented gradient clipping using the torch.nn.utils.clip_grad_norm_ function. I set the function to \"clip\" the norm of the gradients at 1, thereby placing an upper limit on the size of the parameter updates, and preventing them from growing exponentially. After implementation, the scoring trend became more stable and my agent seemed to be able to learn continuously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Experience Replay\n",
    "\n",
    "Experience replay allows the agent to learn from past experience. The difference between this and the previous implementation is that experiences from both agents are stored in a single replay buffer as each agent interacts with the environment. These experiences are then utilized by the central critic, thereby allowing both agents to learn from each others' experiences.\n",
    "\n",
    "The replay buffer contains a collection of experience tuples with the state, action, reward, and next state (s, a, r, s'). The critic samples from this buffer as part of the learning step. Experiences are sampled randomly, so that the data is uncorrelated. This prevents action values from oscillating or diverging catastrophically.\n",
    "\n",
    "Also, experience replay improves learning through repetition. By doing multiple passes over the data, our agents have multiple opportunities to learn from a single experience tuple. This is particularly useful for state-action pairs that occur infrequently within the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Architecture\n",
    "Here's the framework of the code:\n",
    "\n",
    "1. Prepare the Unity environment and import the necessary packages\n",
    "2. Check the Unity environment\n",
    "3. Define functions to instanciate and train multiple DDPG agents\n",
    "4. Train agents using MADDPG framework\n",
    "5. Present results\n",
    "\n",
    "The algo contains actor and critic pair of neural networks. The actor NN uses the following flow:\n",
    "\n",
    "- Input nodes = 24  * 2  = 48 (length of available states * 2)\n",
    "- Fully Connected Layer (256 nodes, Relu activation)\n",
    "- Fully Connected Layer (128 nodes, Relu activation)\n",
    "- Ouput nodes (2 nodes, which is the length of available actions, tanh activation)\n",
    "\n",
    "The Critic NN uses the following flow:\n",
    "\n",
    "- Input nodes = 24  * 2  = 48 (length of available states * 2)\n",
    "- Fully Connected Layer (256 + 2 * 2 = 260 nodes, Relu activation)\n",
    "- Fully Connected Layer (128 nodes, Relu activation)\n",
    "- Ouput node (1 node, no activation)\n",
    "\n",
    "Environment and DDPG Parameters:\n",
    "\n",
    "- state_size. This is the environment state size.\n",
    "- action_size. This is the different actions that the agent can take.\n",
    "- buffer_size. This is how much the cache can store regarding past experiences\n",
    "- batch_size. In the buffer, this is how many samples are being taken out at one time\n",
    "\n",
    "Hyperparameters\n",
    "Model Related: \n",
    "- state_size. This is the environment state size.\n",
    "- action_size. This is the different actions that the agent can take.\n",
    "- buffer_size. This is how much the cache can store regarding past experiences\n",
    "- batch_size. In the buffer, this is how many samples are being taken out at one time\n",
    "- lr_actor. learning rate of the actor NN\n",
    "- lr_critic. learning rate of the critic NN\n",
    "- learn_every. learn ever n episodes\n",
    "- learn_num. number of learning passes\n",
    "- gamma. reward discount factor\n",
    "- tau. Soft Update: weight_target = tau weight_local + (1 - tau) weight_target\n",
    "\n",
    "Noise Related: \n",
    "- add_ounoise. Add Ornstein-Uhlenbeck noise or not?\n",
    "- sigma. Ornstein-Uhlenbeck noise parameter, volatility\n",
    "- theta. Ornstein-Uhlenbeck noise parameter, speed of mean reversion\n",
    "- eps_start. initial value for epsilon in noise decay process\n",
    "- eps_ep_end. episode to end the noise decay process (by this time should not be doing random actions)\n",
    "- eps_final. final value for epsilon after decay\n",
    "\n",
    "\n",
    "Dependencies\n",
    "The libraries that are required to run the code are the following:\n",
    "\n",
    "- unityagents\n",
    "- torch\n",
    "- numpy\n",
    "- random\n",
    "- copy\n",
    "- time\n",
    "- collections\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Given all the setup, the agents were able to break the goal of 0.5 for 100 episodes at the 1610th episode. The moving average score at this point is 0.522, for over 100 episodes.\n",
    "\n",
    "The best moving average of 1.985 was achieved betweeen episodes 1860 and 1870. \n",
    "\n",
    "<img src = 'maddpg_performance_20191108.png'>\n",
    "\n",
    "<img src = 'maddpg_performance_20191108_z.png'>\n",
    "\n",
    "Notice that the score peaked near episode 1860. After that, the average score deteriorated. I would imagine that if I run for more than 2000 episodes, the score would never recover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "\n",
    "One of the difficulties that I faced throughout MADDPG and DDPG exercise is that I don't have any intuition in solving DRL problems, and I have not established an experiene bank on the type of problems versus algos use and hyperparameter values. In order to make my DRL experience as efficient and effective as possible, I feel that I need to have hands on experience on all combinations of the following:\n",
    "\n",
    "- problem type\n",
    "- algorithm type\n",
    "- hyperparameter ranges\n",
    "\n",
    "Otherwise, I feel like I'm always navigating blindly and trying different things without a good reason. Similar to using GridSearch in ML tuning, I would want to spend more time trying to tackle problems with different algos, and different setups, to gain a better idea of how to tackle future problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
